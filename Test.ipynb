{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIL0jeM78tKu"
   },
   "source": [
    "## Student Info\n",
    "\n",
    "* Student name 1:\n",
    "* Student ID 1:\n",
    "\n",
    "* Student name 2:\n",
    "* Student ID 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPGr3YiG80kb"
   },
   "source": [
    "## Project Info\n",
    "\n",
    "This project aims to get hands-on experience in implementing some Classification and clustering algorithms taught in Data Mining and Applications course.\n",
    "\n",
    "There are 2 exercises in this project:\n",
    "* Binary classification using Decision Tree (ID3 algorithm)\n",
    "* Data clustering with K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jeSVc1g-xx0"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "### How to finish\n",
    "\n",
    "To finish the project, fill in:\n",
    "- ```YOUR CODE HERE``` in code cells\n",
    "\n",
    "- ```Your answer here``` in text cells.\n",
    "\n",
    "\n",
    "<font color='red'>**NOTE:**</font>\n",
    "\n",
    "- Your project must be finished by your own self. You may discuss with the others, but must not copy (partially or entirely) their codes or solutions. You will receive a <font color='red'>0 point</font> for this project if you violate this plagiarism rule.\n",
    "\n",
    "- You can create new cells to clarify your code / answer, however, please <font color='red'>do not delete any pre-defined code cells or test case cells</font> as it may affect the grading results.\n",
    "\n",
    "- The given test cases (if any) are used to only assist your code debugging / unit testing. Passing these test cases does not necessarily mean you will get the maximum point for the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSmFr8aou0R1"
   },
   "source": [
    "### How to submit\n",
    "\n",
    "In the grading stage, I will first select `Kernel` - `Restart Kernel & Run All Cells` to restart and run all cells in your notebook. As a result, before submitting your project, you should run `Kernel` - `Restart Kernel & Run All Cells` to ensure your code will run as you expect.\n",
    "\n",
    "After that, rename your notebook as `ID1_ID2.ipynb` (e.g. `19123_19456.ipynb`) and submit on Moodle.\n",
    "\n",
    "<font color=red>Please follow the above submission guidelines. Any violation of these instructions may cost you some penalty points!!!</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BOp6rdqg5SL"
   },
   "source": [
    "## Exercise 1: Classification (6 points)\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to implement Decision Tree algorithm (ID3) in Python language.\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- In this exercise, you only need to build a decision tree model using categorical attributes (no need to handle numerical attributes). You may transform numerical attributes into categorical before feeding into classification model.\n",
    "\n",
    "- A sample belongs to either one of the given labels (binary classification).\n",
    "\n",
    "- Dataset: [tennis.txt](https://drive.google.com/file/d/1jv67IlMIxGxwGGaP47AZ_1t3mxIn82Y7/view?usp=sharing) and [titanic2.txt](https://drive.google.com/file/d/1IwOxcPz-Hq1_JOOfGylJxy1TYhDvfEsg/view?usp=sharing)\n",
    "\n",
    "\n",
    "You will need to implement `load_data` function and DecisionTree class with the following methods:\n",
    "\n",
    "- ```fit```: build tree from given dataset, using ID3 algorithm\n",
    "\n",
    "- ```predict```: make predictions on new data points using the trained decision tree\n",
    "\n",
    "- ```visualize```: plot trained decision tree from training dataset. You may choose how to visualize the trained model, as long as the tree structure of the model is clarified (you may use available visualization packages such as ```tree``` or ```graphviz```, or implement the visualization code yourself). For example, a simple representation of a decision tree built from [tennis.txt](https://drive.google.com/file/d/1jv67IlMIxGxwGGaP47AZ_1t3mxIn82Y7/view?usp=sharing) dataset may look like:\n",
    "\n",
    "\n",
    "```\n",
    "outlook = sunny\n",
    "|  humidity = high: no\n",
    "|  humidity = normal: yes\n",
    "outlook = overcast: yes\n",
    "outlook = rainy\n",
    "|  windy = TRUE: no\n",
    "|  windy = FALSE: yes\n",
    "```\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- For [tennis.txt](https://drive.google.com/file/d/1jv67IlMIxGxwGGaP47AZ_1t3mxIn82Y7/view?usp=sharing) dataset, you can use all samples for training without spliting into train and test set. For [titanic2.txt](https://drive.google.com/file/d/1IwOxcPz-Hq1_JOOfGylJxy1TYhDvfEsg/view?usp=sharing) dataset, you may need to split into train/test with the ratio of 80/20 (use ```sklearn.model_selection.train_test_split``` with ```random_state=520``` for reproducibility)\n",
    "\n",
    "- ```load_data``` function and ```DecisionTree``` class must be implemented once only and be usable for different datasets\n",
    "\n",
    "- For each dataset, you need to ```load_data``` from input files, ```fit``` train dataset into model, use the trained model to ```predict``` on ```X_train``` and ```X_test``` (if any), calculate accuracy (use ```sklearn.metrics.accuracy_score```), and eventually ```visualize``` trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8bxZvhd80Kt"
   },
   "source": [
    "### Implementation (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "zp2i5B0R8m_T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2 as log\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "cttFBEpc9WUX"
   },
   "outputs": [],
   "source": [
    "def load_data(file_path: str, split: bool = True):\n",
    "    \"\"\"\n",
    "    Load data from file_path and return numpy data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path of input data file (tab separated).\n",
    "    split : bool\n",
    "        Whether or not to return test set.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    (X_train, y_train) if split = False, else\n",
    "    (X_train, y_train), (X_test, y_test): training and testing numpy array\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # get feature names from the first row\n",
    "    feature_names = lines[0].strip().split('\\t')[:-1]\n",
    "\n",
    "    # get training data from the second row onwards\n",
    "    data = [line.strip().split('\\t') for line in lines[1:]]\n",
    "    X_train = np.array(data)[:, :-1]\n",
    "    y_train = np.array(data)[:, -1]\n",
    "\n",
    "    # Encode target variable y_train to binary values\n",
    "    y_train = np.array([1 if label == 'yes' else 0 for label in y_train])\n",
    "\n",
    "    # One-hot encode the features\n",
    "    encodings = []\n",
    "    for i in range(X_train.shape[1]):\n",
    "        unique_values = np.unique(X_train[:, i])\n",
    "        encoding = dict(zip(unique_values, range(len(unique_values))))\n",
    "        encodings.append(encoding)\n",
    "        for k, row in enumerate(X_train):\n",
    "            X_train[k][i] = encoding[row[i]]\n",
    "\n",
    "    X_train = X_train.astype(int)\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "    # Split into train and test set\n",
    "    if split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=520)\n",
    "        return (X_train, y_train), (X_test, y_test), feature_names\n",
    "    else:\n",
    "        return X_train, y_train, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(counts, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    counts: shape (n_classes): list number of samples in each class\n",
    "    n_samples: number of data samples\n",
    "    \n",
    "    -----------\n",
    "    return entropy \n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    entropy = 0\n",
    "    \n",
    "    for c in counts: \n",
    "        entropy +=  - (c*1.0/n_samples)*math.log(c*1.0/n_samples, 2)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_of_one_division(division): \n",
    "    \"\"\"\n",
    "    Returns entropy of a divided group of data\n",
    "    Data may have multiple classes\n",
    "    \"\"\"\n",
    "    n_samples = len(division)\n",
    "    n_classes = set(division)\n",
    "    \n",
    "    counts=[]\n",
    "    #count samples in each class then store it to list counts\n",
    "    #TODO:\n",
    "    for c in n_classes:\n",
    "        counts.append(sum(division==c))\n",
    "    \n",
    "    return entropy(counts,n_samples),n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(y_predict, y):\n",
    "    \"\"\"\n",
    "    Returns entropy of a split\n",
    "    y_predict is the split decision by cutoff, True/Fasle\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    entropy_true, n_true = entropy_of_one_division(y[y_predict]) # left hand side entropy\n",
    "    entropy_false, n_false = entropy_of_one_division(y[~y_predict]) # right hand side entropy\n",
    "    # overall entropy\n",
    "    #TODO s=?\n",
    "\n",
    "    s = (n_true*1.0/n)*entropy_true + (n_false*1.0/n)*entropy_false\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "oZlGLcBn-wTM"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, tree=None, depth=0, feature_names=None):\n",
    "        '''Parameters:\n",
    "        -----------------\n",
    "        tree: decision tree\n",
    "        depth: depth of decision tree after training'''\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.tree=tree\n",
    "        self.feature_names=feature_names\n",
    "    def fit(self, X, y, node={}, depth=0):\n",
    "        '''Parameter:\n",
    "        -----------------\n",
    "        X: training data\n",
    "        y: label of training data\n",
    "        ------------------\n",
    "        return: node \n",
    "        \n",
    "        node: each node represented by cutoff value and column index, value and children.\n",
    "         - cutoff value is thresold where you divide your attribute\n",
    "         - column index is your data attribute index\n",
    "         - value of node is mean value of label indexes, \n",
    "           if a node is leaf all data samples will have same label\n",
    "        \n",
    "        Note that: we divide each attribute into 2 part => each node will have 2 children: left, right.\n",
    "        '''\n",
    "        \n",
    "        #Stop conditions\n",
    "        #if all value of y are the same \n",
    "        if np.all(y==y[0]):\n",
    "            return {'val':y[0]}\n",
    "\n",
    "        else: \n",
    "            col_idx, cutoff, entropy = self.find_best_split_of_all(X, y)    # find one split given an information gain \n",
    "            y_left = y[X[:, col_idx] < cutoff]\n",
    "            y_right = y[X[:, col_idx] >= cutoff]\n",
    "            node = {'index_col':col_idx,\n",
    "                        'cutoff':cutoff,\n",
    "                   'val':np.mean(y)}\n",
    "            node['left'] = self.fit(X[X[:, col_idx] < cutoff], y_left, {}, depth+1)\n",
    "            node['right'] = self.fit(X[X[:, col_idx] >= cutoff], y_right, {}, depth+1)\n",
    "            self.depth += 1 \n",
    "            self.tree = node\n",
    "            return node\n",
    "    \n",
    "    def find_best_split_of_all(self, X, y):\n",
    "        col_idx = None\n",
    "        min_entropy = 1\n",
    "        cutoff = None\n",
    "        for i, col_data in enumerate(X.T):\n",
    "            entropy, cur_cutoff = self.find_best_split(col_data, y)\n",
    "            if entropy == 0:                   #best entropy\n",
    "                return i, cur_cutoff, entropy\n",
    "            elif entropy <= min_entropy:\n",
    "                min_entropy = entropy\n",
    "                col_idx = i\n",
    "                cutoff = cur_cutoff\n",
    "               \n",
    "        return col_idx, cutoff, min_entropy\n",
    "    \n",
    "    def find_best_split(self, col_data, y):\n",
    "        ''' Parameters:\n",
    "        -------------\n",
    "        col_data: data samples in column'''\n",
    "         \n",
    "        min_entropy = 10\n",
    "        \n",
    "        for value in set(col_data):\n",
    "            y_predict = col_data < value\n",
    "            my_entropy = get_entropy(y_predict, y)\n",
    "            #TODO\n",
    "            #min entropy=?, cutoff=?\n",
    "            if (min_entropy > my_entropy): \n",
    "                min_entropy = my_entropy \n",
    "                cutoff = value\n",
    "            \n",
    "        return min_entropy, cutoff\n",
    "                                               \n",
    "    def predict(self, X):\n",
    "        tree = self.tree\n",
    "        pred = np.zeros(shape=len(X))\n",
    "        for i, c in enumerate(X):\n",
    "            pred[i] = self._predict(c)\n",
    "        return pred\n",
    "    \n",
    "    def _predict(self, row):\n",
    "        cur_layer = self.tree\n",
    "        while cur_layer.get('cutoff'):\n",
    "            if row[cur_layer['index_col']] < cur_layer['cutoff']:\n",
    "                cur_layer = cur_layer['left']\n",
    "            else:\n",
    "                cur_layer = cur_layer['right']\n",
    "        else:\n",
    "            return cur_layer.get('val')\n",
    "        \n",
    "    def visualize_tree(self):\n",
    "        self.print_tree(self.tree)\n",
    "        \n",
    "    def print_tree(node, depth=0):\n",
    "        if node is None:\n",
    "            return\n",
    "        if node.get('cutoff'):\n",
    "            print(f\"X{node['index_col']} < {node['cutoff']}\")\n",
    "            print_tree(node.get('left'), depth+1)\n",
    "            print_tree(node.get('right'), depth+1)\n",
    "        else:\n",
    "            print(f\"Class: {node['val']}\")\n",
    "\n",
    "        \n",
    "    # previous code...\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "_GQ7FhTQ_zQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0 1]\n",
      " [2 1 0 0]\n",
      " [0 1 0 1]\n",
      " [1 2 0 1]\n",
      " [1 0 1 1]\n",
      " [1 0 1 0]\n",
      " [0 0 1 0]\n",
      " [2 2 0 1]\n",
      " [2 0 1 1]\n",
      " [1 2 1 1]\n",
      " [2 2 1 0]\n",
      " [0 2 0 0]\n",
      " [0 1 1 1]\n",
      " [1 2 0 0]]\n",
      "[0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n",
      "{'index_col': 0, 'cutoff': 1, 'val': 0.6428571428571429, 'left': {'val': 1}, 'right': {'index_col': 2, 'cutoff': 1, 'val': 0.5, 'left': {'index_col': 0, 'cutoff': 2, 'val': 0.2, 'left': {'index_col': 3, 'cutoff': 1, 'val': 0.5, 'left': {'val': 0}, 'right': {'val': 1}}, 'right': {'val': 0}}, 'right': {'index_col': 3, 'cutoff': 1, 'val': 0.8, 'left': {'index_col': 0, 'cutoff': 2, 'val': 0.5, 'left': {'val': 0}, 'right': {'val': 1}}, 'right': {'val': 1}}}}\n",
      "1.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeClassifier' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m acc_train \u001b[39m=\u001b[39m accuracy_score(y_train, y_hat_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(acc_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tree\u001b[39m.\u001b[39;49mvisualize_tree()\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 13\u001b[0m in \u001b[0;36mDecisionTreeClassifier.visualize_tree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize_tree\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprint_tree(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree)\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 13\u001b[0m in \u001b[0;36mDecisionTreeClassifier.print_tree\u001b[1;34m(node, depth)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mif\u001b[39;00m node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m{\u001b[39;00mnode[\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m < \u001b[39m\u001b[39m{\u001b[39;00mnode[\u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X12sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     print_tree(node\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m), depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecisionTreeClassifier' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# 0.5 = 0.25 (tennis dataset) + 0.25 (titanic2 dataset) \n",
    "\n",
    "### NOTE: Flow to run your code (do this for all your datasets)\n",
    "\n",
    "# dataset 1 (create one cell for each dataset with the following content)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "X_train, y_train,name = load_data(\"data/tennis.txt\",split=False)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "tree.fit(X_train, y_train,name)\n",
    "print(tree.fit(X_train, y_train))\n",
    "y_hat_train = tree.predict(X_train) \n",
    "acc_train = accuracy_score(y_train, y_hat_train)\n",
    "print(acc_train)\n",
    "tree.visualize_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [3 0 1]\n",
      " [3 0 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [2 1 1]\n",
      " [1 0 1]]\n",
      "[0 0 0 ... 1 0 0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(y_train)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(tree\u001b[39m.\u001b[39;49mfit(X_train, y_train))\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 14\u001b[0m in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, node, depth)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_right \u001b[39m=\u001b[39m y[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m<\u001b[39;49m cutoff], y_left, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff], y_right, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 14\u001b[0m in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, node, depth)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_right \u001b[39m=\u001b[39m y[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m<\u001b[39;49m cutoff], y_left, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff], y_right, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n",
      "    \u001b[1;31m[... skipping similar frames: DecisionTreeClassifier.fit at line 40 (1 times)]\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 14\u001b[0m in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, node, depth)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_right \u001b[39m=\u001b[39m y[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m<\u001b[39;49m cutoff], y_left, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff], y_right, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 14\u001b[0m in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, node, depth)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m'''Parameter:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m-----------------\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mX: training data\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mNote that: we divide each attribute into 2 part => each node will have 2 children: left, right.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#Stop conditions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#if all value of y are the same \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(y\u001b[39m==\u001b[39my[\u001b[39m0\u001b[39;49m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:y[\u001b[39m0\u001b[39m]}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X45sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melse\u001b[39;00m: \n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "(X_train, y_train), (X_test, y_test),name = load_data(\"data/titanic2.txt\",split=True)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(tree.fit(X_train, y_train))\n",
    "# y_hat_train = tree.predict(X_train) \n",
    "# acc_train = accuracy_score(y_train, y_hat_train)\n",
    "# y_hat_test = tree.predict(X_test) \n",
    "# acc_test = accuracy_score(y_test, y_hat_test)\n",
    "# tree.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entropy(y):\n",
    "    entropy = 0\n",
    "    values = np.unique(y)\n",
    "    for value in values:\n",
    "        fraction = np.sum(y == value) / len(y)\n",
    "        entropy += -fraction * np.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "def find_entropy_attribute(X, y, attribute):\n",
    "    values = np.unique(X[:, attribute])\n",
    "    entropy2 = 0\n",
    "    for value in values:\n",
    "        entropy = find_entropy(y[X[:, attribute] == value])\n",
    "        fraction = np.sum(X[:, attribute] == value) / len(y)\n",
    "        entropy2 += -fraction * entropy\n",
    "    return abs(entropy2)\n",
    "\n",
    "def find_winner(X, y):\n",
    "    information_gains = []\n",
    "    for i in range(X.shape[1]):\n",
    "        information_gain = find_entropy(y) - find_entropy_attribute(X, y, i)\n",
    "        information_gains.append(information_gain)\n",
    "    max_gain_index = np.argmax(information_gains)\n",
    "    return max_gain_index\n",
    "\n",
    "\n",
    "def get_subtable(X, y, node, value):\n",
    "    X_subset = X[X[:, node] == value]\n",
    "    y_subset = y[X[:, node] == value]\n",
    "    return np.delete(X_subset, node, axis=1), y_subset\n",
    "\n",
    "def buildTree(X, y, feature_names, tree=None):\n",
    "    if tree is None:\n",
    "        tree = {}\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return y[0]\n",
    "    if X.shape[1] == 0:\n",
    "        return np.argmax(np.bincount(y))\n",
    "    node = find_winner(X, y)\n",
    "    print(node)\n",
    "    node_name = feature_names[node]\n",
    "    print(feature_names[node])\n",
    "    tree[node_name] = {}\n",
    "    for value in np.unique(X[:, node]):\n",
    "        X_subset, y_subset = get_subtable(X, y, node, value)\n",
    "        if X_subset.shape[1] == 0:\n",
    "            tree[node_name][value] = np.argmax(np.bincount(y_subset))\n",
    "        else:\n",
    "            tree[node_name][value] = buildTree(X_subset, y_subset, feature_names)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['outlook', 'temperature', 'humidity', 'wind']\n",
      "0\n",
      "outlook\n",
      "2\n",
      "humidity\n",
      "1\n",
      "temperature\n",
      "{'outlook': {0: 1,\n",
      "             1: {'humidity': {0: 0, 1: 1}},\n",
      "             2: {'temperature': {0: 0, 1: 1}}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "X_train, y_train, feature_names = load_data('data/tennis.txt', split=False)\n",
    "print(feature_names)\n",
    "tree=buildTree(X_train,y_train,feature_names)\n",
    "pprint.pprint(tree)\n",
    "# Convert the input data to numerical values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:272: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ar = np.asanyarray(ar)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,0) into shape (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tree\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m X,y,_\u001b[39m=\u001b[39m load_data(\u001b[39m'\u001b[39m\u001b[39mtennis.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m tree \u001b[39m=\u001b[39m buildTree(X, y,_)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(tree)\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\University\\DataMining\\Lab03\\Test.ipynb Cell 17\u001b[0m in \u001b[0;36mbuildTree\u001b[1;34m(X, y, feature_names, tree)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m tree \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     tree \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39;49munique(y)) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/University/DataMining/Lab03/Test.ipynb#X51sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:272\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_unique_dispatcher)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munique\u001b[39m(ar, return_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_inverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    140\u001b[0m            return_counts\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, equal_nan\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    141\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Find the unique elements of an array.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masanyarray(ar)\n\u001b[0;32m    273\u001b[0m     \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m         ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                         equal_nan\u001b[39m=\u001b[39mequal_nan)\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (3,0) into shape (3,)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "def find_entropy(y):\n",
    "    entropy = 0\n",
    "    values = np.unique(y)\n",
    "    for value in values:\n",
    "        fraction = np.sum(y == value) / len(y)\n",
    "        entropy += -fraction * np.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "def find_entropy_attribute(X, y, attribute):\n",
    "    values = np.unique(X[:, attribute])\n",
    "    entropy2 = 0\n",
    "    for value in values:\n",
    "        entropy = find_entropy(y[X[:, attribute] == value])\n",
    "        fraction = np.sum(X[:, attribute] == value) / len(y)\n",
    "        entropy2 += -fraction * entropy\n",
    "    return abs(entropy2)\n",
    "\n",
    "def find_winner(X, y):\n",
    "    information_gains = []\n",
    "    for i in range(X.shape[1]):\n",
    "        information_gain = find_entropy(y) - find_entropy_attribute(X, y, i)\n",
    "        information_gains.append(information_gain)\n",
    "    max_gain_index = np.argmax(information_gains)\n",
    "    return max_gain_index\n",
    "\n",
    "\n",
    "def get_subtable(X, y, node, value):\n",
    "    X_subset = X[X[:, node] == value]\n",
    "    y_subset = y[X[:, node] == value]\n",
    "    return np.delete(X_subset, node, axis=1), y_subset\n",
    "\n",
    "\n",
    "\n",
    "def buildTree(df, tree=None):\n",
    "    if tree is None:\n",
    "        tree = {}\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return y[0]\n",
    "    if X.shape[1] == 0:\n",
    "        return np.argmax(np.bincount(y))\n",
    "    node = find_winner(X, y)\n",
    "    node_name = feature_names[node]\n",
    "    tree[node_name] = {}\n",
    "    values = np.unique(X[:, node])\n",
    "    for value in values:\n",
    "        X_subset, y_subset = get_subtable(X, y, node, value)\n",
    "        if len(np.unique(y_subset)) == 1:\n",
    "            tree[node_name][value] = y_subset[0]\n",
    "        else:\n",
    "            subtree = buildTree(X_subset, y_subset, feature_names)\n",
    "            tree[node_name][value] = subtree\n",
    "    if node_name not in ['Outlook', 'Wind', 'Humidity', 'Temperature']:\n",
    "        return tree[node_name][values[0]]\n",
    "    return tree\n",
    "\n",
    "\n",
    "X,y,_= load_data('tennis.csv')\n",
    "\n",
    "tree = buildTree(X, y,_)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTreee:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        \n",
    "    def find_entropy(self, df):\n",
    "        Class = df.keys()[-1]\n",
    "        entropy = 0\n",
    "        values = df[Class].unique()\n",
    "        for value in values:\n",
    "            fraction = df[Class].value_counts()[value]/len(df[Class])\n",
    "            entropy += -fraction*np.log2(fraction)\n",
    "        return entropy\n",
    "    \n",
    "    def find_entropy_attribute(self, df, attribute):\n",
    "        Class = df.keys()[-1]\n",
    "        target_variables = df[Class].unique()\n",
    "        variables = df[attribute].unique()\n",
    "        entropy2 = 0\n",
    "        for variable in variables:\n",
    "            entropy = 0\n",
    "            for target_variable in target_variables:\n",
    "                num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
    "                den = len(df[attribute][df[attribute]==variable])\n",
    "                fraction = num/(den+np.finfo(float).eps)\n",
    "                entropy += -fraction*np.log2(fraction+np.finfo(float).eps)\n",
    "            fraction2 = den/len(df)\n",
    "            entropy2 += -fraction2*entropy\n",
    "        return abs(entropy2)\n",
    "    \n",
    "    def find_winner(self, df):\n",
    "        IG = []\n",
    "        for key in df.keys()[:-1]:\n",
    "            IG.append(self.find_entropy(df)-self.find_entropy_attribute(df,key))\n",
    "        return df.keys()[:-1][np.argmax(IG)] \n",
    "    \n",
    "    def get_subtable(self, df, node, value):\n",
    "        return df[df[node] == value].reset_index(drop=True)\n",
    "    \n",
    "    def buildTree(self,df, tree=None): \n",
    "        Class = df.keys()[-1]\n",
    "        node = self.find_winner(df)\n",
    "        attValue = np.unique(df[node])\n",
    "        print(attValue)\n",
    "        if tree is None:                    \n",
    "            tree={}\n",
    "            tree[node] = {}\n",
    "        for value in attValue:\n",
    "            subtable = self.get_subtable(df,node,value)\n",
    "            clValue,counts = np.unique(subtable[df.keys()[-1]],return_counts=True)                        \n",
    "            if len(counts)==1:\n",
    "                tree[node][value] = clValue[0]                                                    \n",
    "            else:        \n",
    "                tree[node][value] = self.buildTree(subtable)               \n",
    "        return tree\n",
    "    def fit(self,df):\n",
    "        self.tree=buildTree(df,self.tree)\n",
    "    \n",
    "    # def predict(self, X):\n",
    "    #     y_pred = []\n",
    "    #     for row in X:\n",
    "    #         y_pred.append(self._predict_row(row, self.tree))\n",
    "    #     return y_pred\n",
    "    \n",
    "    # def _predict_row(self, row, tree):\n",
    "    #     for node, subtree in tree.items():\n",
    "    #         value = row[node]\n",
    "    #         if value not in subtree:\n",
    "    #             return None\n",
    "    #         if isinstance(subtree[value], dict):\n",
    "    #             return self._predict_row(row, subtree[value])\n",
    "    #         else:\n",
    "    #             return subtree[value]\n",
    "                \n",
    "    def visualize(self):\n",
    "        pprint.pprint(self.tree)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Overcast' 'Rain' 'Sunny']\n",
      "['Strong' 'Weak']\n",
      "['High' 'Normal']\n",
      "{'Outlook': {'Overcast': 'Yes', 'Rain': None, 'Sunny': None}}\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreee()\n",
    "X_train, y_train,temp= load_data(\"data/tennis.txt\",split=False)\n",
    "# train_data = np.column_stack((X_train, y_train.reshape(-1,1)))\n",
    "# df = pd.DataFrame(train_data, columns=feature_names + [\"class\"])\n",
    "df = pd.read_csv('tennis.csv')\n",
    "tree.fit(df)\n",
    "# y_hat_train = tree.predict(X_train) \n",
    "# acc_train = accuracy_score(y_train, y_hat_train)\n",
    "tree.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.754887502163468\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkJf1OrO_ijl"
   },
   "source": [
    "### Answer the following question (0.5 points)\n",
    "\n",
    "**Question 1:** Your comments on the accuracy scores between train and test sets on ```titanic2``` dataset? Elaborate, explain and propose a solution\n",
    "\n",
    "---\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gXUwu_vDmep"
   },
   "source": [
    "## Exercise 2: Clustering (4 points + 1 bonus point)\n",
    "\n",
    "### Requirements\n",
    "\n",
    "In this exercise, you will need to implement K-means clustering algorithm on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U9MTEeMP_xbj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w5S-iHLJFRx"
   },
   "source": [
    "**Dataset:**\n",
    "\n",
    "- The synthetic dataset contains 1500 data points that belong to 3 clusters. The clusters' centers are respectively $(2,2), (7,3), (3,6)$. \n",
    "\n",
    "- Data points of each cluster are generated randomly from the bivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OW6Xu17kMIky"
   },
   "outputs": [],
   "source": [
    "seed = 520 # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8MEYeXCQH1Dr"
   },
   "outputs": [],
   "source": [
    "### Description: generate synthetic data\n",
    "\n",
    "def generate_data(means, cov, N, K):\n",
    "    np.random.seed(seed) \n",
    "    X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "    X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "    X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "\n",
    "    X = np.concatenate((X0, X1, X2), axis = 0)\n",
    "    original_label = np.asarray([0]*N + [1]*N + [2]*N).T\n",
    "\n",
    "    return X, original_label\n",
    "\n",
    "\n",
    "# visualize data \n",
    "def kmeans_display(X, label):\n",
    "    K = np.amax(label) + 1\n",
    "    X0 = X[label == 0, :]\n",
    "    X1 = X[label == 1, :]\n",
    "    X2 = X[label == 2, :]\n",
    "    \n",
    "    plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\n",
    "    plt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\n",
    "    plt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "V3sqpASYLVd6",
    "outputId": "180ecf12-50ae-42d2-b9c0-0967b24e2222"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N, K = 500, 3\n",
    "means = [[2, 2], [7, 3], [3, 6]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "\n",
    "X, original_label = generate_data(means, cov, N, K)\n",
    "kmeans_display(X, original_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wDWbfKgLhcp"
   },
   "source": [
    "You will need to fill in the ```YOUR CODE HERE``` to finish K-means clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGyHXCaeLy64"
   },
   "source": [
    "### Implementation (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-pOgiA5_ME9w"
   },
   "outputs": [],
   "source": [
    "# 0.5\n",
    "def kmeans_init_centers(X, k):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # randomly pick k rows of X as initial centers using np.random.choice function\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEtg0cx6MVJY"
   },
   "outputs": [],
   "source": [
    "centers = kmeans_init_centers(X, K)\n",
    "assert centers.shape == (K, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uE76wVmMVk5"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "def kmeans_assign_labels(X, centers):\n",
    "    # calculate pairwise distances between data and centers using cdist\n",
    "    # return index of the closest center using np.argmin\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dejjjGM8MZMJ"
   },
   "outputs": [],
   "source": [
    "assigned_labels = kmeans_assign_labels(X, centers)\n",
    "assert assigned_labels.shape == (N*3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NI48jo7IMa20"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "def kmeans_update_centers(X, labels, K):\n",
    "    centers = np.zeros((K, X.shape[1]))\n",
    "    for k in range(K):\n",
    "        # collect all points assigned to the k-th cluster \n",
    "        # take average\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sB4HwTURMdic"
   },
   "outputs": [],
   "source": [
    "# 0.5\n",
    "def has_converged(centers, new_centers):\n",
    "    # return True if two sets of centers are the same\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bcLqYi_MfxM"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "def kmeans(X, K):\n",
    "    # save the center coordinates of each iteration\n",
    "    centers = [kmeans_init_centers(X, K)]  \n",
    "    # save the labels of each iteration\n",
    "    labels = []\n",
    "    it = 0 \n",
    "    while True:\n",
    "        # at each iteration:\n",
    "        # 1. assign label for each points and append to labels\n",
    "        # 2. update the centers\n",
    "        # 3. check the convergence condition\n",
    "        #    and append NEW center coordinates to centers\n",
    "        # 4. update iteration \n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return (centers, labels, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAPz0ii1Mh94"
   },
   "outputs": [],
   "source": [
    "(centers, labels, it) = kmeans(X, K)\n",
    "print('Centers found by k-means algorithm:')\n",
    "print(centers[-1])\n",
    "print('='*60)\n",
    "\n",
    "kmeans_display(X, labels[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1_S2KR3L72g"
   },
   "source": [
    "### Answer the following questions (bonus 1 point)\n",
    "\n",
    "**Question 2:** Your comments on the result of K-means algorithm on the synthetic dataset? (0.25)\n",
    "\n",
    "---\n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7lsqqBKMBRE"
   },
   "source": [
    "**Question 3:** \n",
    "\n",
    "- Drawbacks of K-means clustering algorithm (0.5)\n",
    "\n",
    "- Propose solutions to the drawbacks you stated above (0.25)\n",
    "\n",
    "---\n",
    "\n",
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
